# **RAG (Retrieval-Augmented Generation)**

Este projeto implementa um sistema de **Gera√ß√£o Aumentada por Recupera√ß√£o (RAG)**, que combina **Modelos de Linguagem Grande (LLMs)** com dados externos para fornecer respostas precisas, atualizadas e contextuais.

---

## **Descri√ß√£o**

Um pipeline RAG √© projetado para responder perguntas ao recuperar informa√ß√µes de fontes externas e process√°-las com um LLM. Este projeto utiliza o **LangChain**, uma biblioteca voltada para a cria√ß√£o de fluxos de trabalho com LLMs, para construir o pipeline.

### **Etapas do Pipeline**

1. **Ingest√£o de Dados**  
   O processo come√ßa com a coleta de dados de diversas fontes, como arquivos de texto, PDFs e p√°ginas da web. O LangChain oferece ferramentas espec√≠ficas para isso, incluindo:
   - **TextLoader**: Carrega arquivos de texto.
   - **PyPDFLoader**: L√™ dados de documentos PDF.

2. **Transforma√ß√£o de Dados**  
   Ap√≥s a ingest√£o, os dados s√£o preparados para uso. Uma etapa comum √© a divis√£o dos documentos em trechos menores, devido √† limita√ß√£o do tamanho de contexto dos LLMs.  
   Ferramenta recomendada: **RecursiveCharacterTextSplitter**.

3. **Incorpora√ß√£o de Vetores**  
   Os dados s√£o convertidos em representa√ß√µes vetoriais usando m√©todos como **OpenAIEmbeddings** ou **OllamaEmbeddings**. Esses vetores s√£o armazenados em reposit√≥rios especializados, como:
   - **FAISS**: R√°pido e eficiente, ideal para busca de vetores similares, mas exige configura√ß√£o detalhada.

### **Benef√≠cios**
- **Respostas atualizadas** com dados externos.
- **Menor depend√™ncia de modelos gigantes**, j√° que o conhecimento √© buscado dinamicamente.
- **Versatilidade de aplica√ß√£o** em √°reas como suporte ao cliente, pesquisa acad√™mica e an√°lise jur√≠dica.

---

## **Funcionalidades**
- Busca em bases de dados externas com motores vetoriais (e.g., FAISS, Pinecone, Weaviate).
- Integra√ß√£o com LLMs como GPT, LLaMA ou modelos dispon√≠veis no Hugging Face.
- Recupera√ß√£o eficiente para encontrar documentos relevantes com embeddings.
- Gera√ß√£o din√¢mica de respostas com base nos dados recuperados.

---

## **Requisitos**
- **Python 3.8+**
- Depend√™ncias principais:
  - `transformers`
  - `faiss-cpu` (ou outro motor vetorial)
  - `sentence-transformers`
  - `torch`
- Veja mais detalhes em `requirements.txt`.

---

## **Instala√ß√£o**
1. Clone o reposit√≥rio:
   ```bash
   git clone https://github.com/seu_usuario/llama_rag.git
   cd llama_rag
   ```
2. Instale as depend√™ncias:
   ```bash
   pip install -r requirements.txt
   ```
3. Configure os dados:
   - Adicione seus arquivos no diret√≥rio `data/`.
   - Gere os embeddings:
     ```bash
     python create_embeddings.py
     ```

---

## **Como usar**
1. Inicie o servidor:
   ```bash
   python app.py
   ```

2. Acesse a interface ou envie requisi√ß√µes para gerar respostas baseadas nos dados fornecidos.

---

## **Estrutura do Projeto**
```plaintext
‚îú‚îÄ‚îÄ data/                 # Diret√≥rio para arquivos de dados
‚îú‚îÄ‚îÄ embeddings/           # Armazenamento de embeddings gerados
‚îú‚îÄ‚îÄ models/               # Modelos e configura√ß√µes
‚îú‚îÄ‚îÄ app.py                # Script principal para execu√ß√£o
‚îú‚îÄ‚îÄ create_embeddings.py  # Gera√ß√£o de embeddings
‚îú‚îÄ‚îÄ requirements.txt      # Lista de depend√™ncias
‚îú‚îÄ‚îÄ README.md             # Documento de instru√ß√µes
```

---

## **Exemplo de Uso**

- **Entrada**: Uma pergunta feita pelo usu√°rio.  
- **Processo**: O pipeline recupera informa√ß√µes relevantes, processa os dados com o LLM e retorna uma resposta clara e contextualizada.  

---

## **Contribui√ß√µes**
Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou enviar pull requests para aprimorar o projeto.

---

## **Licen√ßa**
Este projeto est√° licenciado sob a **MIT License**. Consulte o arquivo `LICENSE` para mais detalhes. 

---

Precisa de ajustes adicionais? Estou √† disposi√ß√£o! üöÄ
